<body style="background-color: #FBD5EF;">
<FONT color='#D3177E'><FONT size=6>

::: {align="center"}
# Projet d‚Äô√©tude du jeu de donn√©es FAT
:::
<hr style="border: 1px solid #D3177E;">
</FONT></FONT>

<FONT color='#D3177E'><FONT size=4>

::: {align="center"}
**Eve BENSOUSSAN et L√©hna BOUCHAMA**  
**E4 Fili√®re e-Sant√© ESIEE Paris**


*Ann√©e 2024 - 2025*
:::

</FONT></FONT>



<hr style="border: 1px solid #D3177E;">

#### <FONT color='#EE50A3'> I. Introduction  </FONT>
  
Dans ce projet, on analyse le jeu de donn√©es **FAT** extrait du fichier **FAT.xlsx** en utilisant des techniques de r√©gression lin√©aire, mais aussi des mod√®les p√©nalis√©s comme Ridge et Lasso. Le but est de comprendre les relations entre les variables et de pr√©dire le pourcentage de graisse corporelle calcul√© avec la m√©thode Brozek.

Pour cela, on suit plusieurs √©tapes : estimer les param√®tres, valider le mod√®le, d√©tecter les points influents et g√©rer la multicolin√©arit√©. On compare aussi nos r√©sultats avec ceux des fonctions int√©gr√©es de R, en s'appuyant sur des outils graphiques pour mieux interpr√©ter les donn√©es.

Ce projet permet de mieux ma√Ætriser les m√©thodes de mod√©lisation et d‚Äôanalyse tout en travaillant sur un vrai jeu de donn√©es.

Dans un premier temps, nous allons tester un script permettant de programmer une r√©gression lin√©aire multiple et nous comparerons les r√©sultats obtenus avec ceux fournis pas les fonctions standards de R. Dans un second temps, si la r√©gression lin√©aire ne convient pas nous r√©aliserons une r√©gression p√©nalis√©e.

Nous utilisons les biblioth√®ques R suivantes :

- **here** : pour g√©rer les r√©pertoires de mani√®re flexible,
- **ggplot2** : pour visualiser les r√©sultats,
- **kableExtra** : pour mettre en forme les tableaux,
- **lmtest** : pour r√©aliser le test de Durbin-Watson,
- **GGally** : pour cr√©er des graphiques pair√©s,
- **corrplot** : pour visualiser les matrices de corr√©lation,
- **car** : pour calculer le facteur d'inflation de la variance,
- **readxl** : pour importer des fichiers Excel, comme `FAT.xlsx`,
- **glmnet** : pour impl√©menter des mod√®les de r√©gression p√©nalis√©e (Ridge et Lasso),
- **dplyr** : pour manipuler et transformer les donn√©es (filtrage, regroupement, etc.),
- **reshape2** : pour restructurer les donn√©es entre les formats large et long,
- **performance** : pour √©valuer la performance et la qualit√© des mod√®les statistiques,
- **see** : pour visualiser les graphiques qui vont avec les commandes de performance.

Le jeu de donn√©es analys√©, extrait du fichier **FAT.xlsx**, contient 252 observations r√©parties sur 18 variables.

Les variables sont les suivantes :


- `brozek` : Pourcentage de graisse corporelle estim√© selon l'√©quation de Brozek (457 / densit√© - 414,2)
- `siri` : Pourcentage de graisse corporelle en utilisant l'√©quation de Siri (495 / densit√© - 450)
- `densit√©` : Densit√© en (gm/cm^2)
- `√¢ge` : √Çge en ann√©es
- `poids` : Poids en livres (lbs)
- `hauteur` : Hauteur en pouces
- `adipos` : Indice de masse corporelle (IMC) ou indice d'adiposit√© (kg/m¬≤)
- `libre` : Poids sans graisse (lbs), calcul√© comme (1 - fraction de graisse corporelle) * poids, en utilisant la formule de Brozek
- `cou` : Circonf√©rence du cou en cm
- `poitrine` : Tour de poitrine en cm
- `abdomen` : Circonf√©rence abdominale en cm (mesur√©e au niveau du nombril et de la cr√™te iliaque)
- `hanche` : Tour de hanche en cm
- `dthigh` : Tour de cuisse en cm
- `genou` : Circonf√©rence du genou en cm
- `cheville` : Tour de cheville en cm
- `biceps` : Circonf√©rence √©tendue du biceps en cm
- `avant-bras` : Circonf√©rence de l'avant-bras en cm
- `poignet` : Circonf√©rence du poignet en cm (mesur√©e distale aux apophyses stylo√Ødes)


<hr style="border: 1px solid #EE50A3;">

#### <FONT color='#EE50A3'>II. Analyse des donn√©es et r√©gression lin√©aire multiple</FONT>

##### <u><FONT color='#8A226'>1. Mod√®le de r√©gression lin√©aire multiple</FONT></u>



On repr√©sente les donn√©es et le mod√®le de r√©gression multiple en utilisant bien les √©critures matricielles. Pour rappel :

- \( X \) est une matrice de taille \( (n, p) \) o√π la premi√®re colonne est constitu√©e de 1 pour repr√©senter l'ordonn√©e √† l'origine.
- \( Y \) est un vecteur colonne de taille \( (n, 1) \) repr√©sentant les valeurs observ√©es de la variable d√©pendante.

Les formules ci-dessous utilisent ces notations :

1. **Estimation des param√®tres**  
   Les param√®tres du mod√®le sont estim√©s avec :
   \[
   \hat{\Theta} = (X^T X)^{-1} X^T Y
   \]
   o√π \( \hat{\Theta} \) est le vecteur des coefficients estim√©s, incluant l'ordonn√©e √† l'origine.

2. **Valeurs estim√©es**  
   Les valeurs pr√©dites (ou ajust√©es) sont donn√©es par :
   \[
   \hat{Y} = X \hat{\Theta}
   \]

3. **R√©sidus**  
   La diff√©rence entre les valeurs observ√©es \( Y \) et les valeurs pr√©dites \( \hat{Y} \) repr√©sente les r√©sidus :
   \[
   \text{R√©sidus} = Y - \hat{Y} = Y - X \hat{\Theta}
   \]

4. **Somme des carr√©s des r√©sidus (SCR)**  
   La somme des carr√©s des r√©sidus est calcul√©e par :
   \[
   SCR = (Y - X \hat{\Theta})^T (Y - X \hat{\Theta})
   \]

5. **Variance r√©siduelle**  
   La variance r√©siduelle est donn√©e par :
   \[
   \hat{\sigma}^2 = \frac{SCR}{n - p}
   \]

6. **Variance des param√®tres**  
   La variance des coefficients estim√©s se calcule avec :
   \[
   V(\hat{\Theta}) = \hat{\sigma}^2 (X^T X)^{-1}
   \]

7. **Test de significativit√© des param√®tres**  
   Le test de significativit√© des coefficients utilise la statistique t :
   \[
   T_{obs} = \frac{\hat{\theta}_j}{\sqrt{\hat{\sigma}^2 (X^T X)^{-1}_{jj}}}
   \]
   o√π \( T_{obs} \) suit une loi de Student avec \( n - p \) degr√©s de libert√©.

8. **Matrice Chapeau (Hat Matrix)**  
   La matrice chapeau \( H \) est donn√©e par :
   \[
   H = X (X^T X)^{-1} X^T
   \]

##### <u><FONT color='#8A226'>2. Visualisation et diagnostics</FONT></u>


Nous observons les donn√©es suivantes:
```{r, echo = T, message=F, warning=F}
rm( list =ls())

#chargement des libraries
library(here)

library(kableExtra)
library(lmtest)
library(GGally)
library (corrplot)
library(readxl)
library(car)
library(glmnet)
library(dplyr)
library(reshape2)
library(performance)
library(see)

#chargement du jeu de donn√©e (ChatGPT: "comment charger un jeu de donn√©es excel dans un dataframe sur R")
df<-fat <- read_excel(here('FAT.xlsx')) 

#afficher le jeu de donn√©es
df %>% kbl(digits=3) %>%  kable_styling(bootstrap_options = "striped", full_width = F, position = "center", latex_options = 'stripped')  %>% 
                    scroll_box( height = "250px")  %>%  
                    footnote(general = 'jeu de donn√©es de validation',  footnote_as_chunk = T, title_format = c("italic", "underline"))
```

<br> <hr>

<u>Param√®tres du mod√®le</u>

Tout d'abord nous cherchons √† cr√©er un mod√®le, soit une √©quation pour quantifier la relation entre la variable que nous cherchons √† pr√©dire et les variables explicatives. 

Pour cela nous avons commenc√© par calculer les valeurs estim√©es √† partir de la matrice chapeau. Puis nous avons calcul√© les r√©sidus qui repr√©sentent les erreurs. Nous avons ensuite calcul√© la variabilit√© r√©siduelle SCR, mesur√© la variance r√©siduelle soit la variabilit√© des erreurs. Nous avons ensuite calcul√© la matrice des variances/ covariances des param√®tres. Enfin nous avons appliqu√© des inf√©rences statistiques afin de calculer le tableau ANOVA et de r√©aliser le test de Fisher. (Tout ceci est disponible dans le fichier Markdown.rmd)

Nous cr√©ons notre premier mod√®le sans `Siri` et sans `densit√©` car, il est indiqu√© dans le Excel de pr√©sentation des donn√©es que, tout comme `Brozek` ils mesurent la masse corporelle et sont donc trop corr√©l√©s. 
```{r, echo = F}
# Param√®tres du mod√®le
X <- as.matrix(cbind(1, df[, c("age","weight","height","adipos","free","neck","chest","abdom","hip","thigh","knee","ankle","biceps","forearm","wrist")]))  
# on retire siri et densit√© car ils mesurent aussi l'indice de masse corpo
# (chatgpt: "comment mettre seulement quelques variables dans un param√®tre X dans la regression lin√©aire multiple sur R") 

# Param√®tres du mod√®le suite
Y <- as.matrix(df$brozek) #y = la variable qu'on cherche
n <- nrow(X) # n = colonnes
p <- ncol(X) # p = lignes

# Calcul de Theta (param√®tres de la r√©gression)
Theta_hat <- solve(t(X) %*% X) %*% t(X) %*% Y
rownames(Theta_hat) <- c("Intercept", "age","weight","height","adipos","free","neck","chest","abdom","hip","thigh","knee","ankle","biceps","forearm","wrist")
#Theta_hat (chatgpt: comment calculer theta chapeau sur R en regression lin√©aire multiple)
```
```{r, echo = F}
#Nous commen√ßons par calculer les valeurs estim√©es, √† partir de la matrice chapeau.
#-> valeurs estim√©es

Y_hat <- X %*% Theta_hat
#data.frame(Y_hat) %>% head() %>% kbl(digits=3) %>% kable_styling(bootstrap_options = "striped", full_width = F, position = "center", latex_options = 'stripped') 
```
```{r, echo = F}
#Nous calculons ensuite les r√©sidus
#-> r√©siduelle
eps <- Y - Y_hat

#data.frame(eps) %>% head() %>% kbl(digits=3) %>%  kable_styling(bootstrap_options = "striped", full_width = F, position = "center", latex_options = 'stripped') %>% scroll_box( height = "250px")  
```
```{r, echo = F}
#Puis nous calculons la variabilit√© r√©siduelle : SCR.
SCR <- t(eps) %*% eps
#SCR
```
```{r, echo = F}
#Nous mesurons ensuite la variance r√©siduelle, soit la variabilit√© des erreurs.
sigma2_hat <- SCR / (n - p)
#sigma2_hat
```
```{r, echo = F}
#On calcule ensuite la matrice des variances covariances des param√®tres.
sig_param <- as.numeric(sigma2_hat) * solve(t(X) %*% X)
#data.frame(sig_param) %>% head() %>% kbl(digits=3) %>%  kable_styling(bootstrap_options = "striped", full_width = F, position = "center", latex_options = 'stripped')
```
```{r, echo = F}
#Nous pouvons ensuite calculer les inf√©rences statistiques sur les param√®tres.
#-> Inf√©rence statistique sur les param√®tres
T_obs <- Theta_hat / sqrt(diag(sig_param))
p_values <- 2 * (1 - pt(abs(T_obs), df = n - p))
stat_param <- data.frame(Theta_hat, T_obs, p_values)
colnames(stat_param) <- c("Estimate", "t-value", "p-value")
#stat_param %>% head() %>% kbl(digits=3) %>% kable_styling(bootstrap_options = "striped", full_width = F, position = "center", latex_options = 'stripped')
#data.frame(stat_param) %>% kbl(digits=3) %>%  kable_styling(bootstrap_options = "striped", full_width = F, position = "center", latex_options = 'stripped')
```
```{r, echo = F}
#Nous appliquons ces inf√©rences statistiques sur le mod√®le de r√©gression pour obtenir le tableau d'ANOVA.
H <- X %*% solve(t(X) %*% X) %*% t(X)
#-> SCReg
SCReg <- t(Y_hat - mean(Y)) %*% (Y_hat - mean(Y))
#-> SCT
SCT <- t(Y - mean(Y)) %*% (Y - mean(Y))

#-> Tableau d'ANOVA

Tab_Reg <- data.frame(
  Source = c("R√©gression", "R√©siduelle"),
  SC = c(SCReg, SCR),
  ddl = c(p - 1, n - p),
  CM = c(SCReg / (p - 1), SCR / (n - p))
)
Tab_Reg$F_value <- c(Tab_Reg$CM[1] / Tab_Reg$CM[2], NA)

#Tab_Reg %>% head() %>% kbl(digits=3) %>% kable_styling(bootstrap_options = "striped", full_width = F, position = "center", latex_options = 'stripped')
```
```{r, echo = F}
#Enfin nous utilisons les inf√©rences statistiques sur le mod√®le de r√©gression pour obtenir le test de Fisher.
#-> test de fisher
Fc <- Tab_Reg$F_value[1]
Prob <- 1 - pf(Fc, p - 1, n - p)
#cat('F_obs = ', Fc, '  Proba = ', Prob)
#data.frame(Tab_Reg) %>% kbl(digits=3) %>%  kable_styling(bootstrap_options = "striped", full_width = F, position = "center", latex_options = 'stripped')
```


On cr√©e notre mod√®le √† partir des diff√©rentes variables, puis on regarde le r√©sum√©.
Pour cr√©er notre mod√®le nous utilisons la fonction `lm()`, qui permet de calculer les param√®tres et les inf√©rences associ√©es √† nos variables. 
```{r}

myf <- 'brozek ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist'
modele <- lm(myf, data = df)
summary(modele)
```
Nous pouvons observer le r√©sum√© du mod√®le cherchant √† expliquer la variable `brozek` en fonction des diff√©rentes variables, soit `age`, `weight`, `height`, `adipos`, `free`, `neck`, `chest`, `abdom`, `hip`, `thigh`, `knee`, `ankle`, `biceps`, `forearm` et `wrist`. 

Les p-values sont des valeurs permettant de rejeter l'hypoth√®se nulle. Celle-ci correspond √† l'absence de significativit√© des variables dans le mod√®le, autrement dit que leur coefficient soit nul. Nous observons que les variables avec des √©toiles ou des points sont les seules avec des p-values<0,1 permettant de rejeter l'hypoth√®se nulle et donc d'√™tre statistiquement significatives.  Nous ne gardons donc que ces variables, et supprimons celles avec une p-value au dessus du seuil que nous avons choisi:

```{r}
modele <- lm(brozek ~  weight + adipos + free + chest + abdom + thigh + ankle + biceps + forearm , data = df)
summary(modele)
```
Ici nous observons donc le mod√®le cherchant √† expliquer la variable `brozek`, √† l'aide de `weight`, `adipos`, `free`, `chest`, `abdom`, `thigh`, `ankle`, `biceps` et `forearm.`
Nous voyons dans l'analyse des r√©sidus qu'il y a des valeurs extr√™mes car les r√©sidus maximaux et minimaux sont assez √©loign√©s de 0.
Dans la partie coefficient nous pouvons voir une ligne `intercept`, qui nous indique la valeur moyenne de `brozek` quand les variables sont nulles. (source ChatGPT: "explique intercept dans un summary R regression multiple")
Puis nous v√©rifions que toutes les variables ont des √©toiles, elles contribuent donc toute au mod√®le car elles ont une p-value<0,5. Les plus significatives sont `weight`, `free` avec une p-value inf√©rieure √† 2e-16. 
Nous voyons dans notre `summary` que R¬≤ est √©gal a 0,96 nous avons donc 96% de variance expliqu√©e.


Nous avons √©galement mesur√© les intervalles de confiances associ√©s aux param√®tres.
```{r}
pr√©diction <-predict(modele, interval = 'confidence') 
pr√©diction %>% head()%>% kbl(digits=3) %>% kable_styling(bootstrap_options = "striped", full_width = F, position = "center", latex_options = 'stripped')
```

Nous voyons 3 colonnes : 


 - `fit` : la valeur pr√©dite
 - `lwr` : le seuil planch√© de l'intervalle de confiance autour de la pr√©diction
 - `upr` : le seuil plafond de ce m√™me intervalle de confiance.
 
On voit que les intervalles de pr√©cisions sont tr√®s faibles, donc les valeurs pr√©dites sont assez pr√©cises. 

<br><hr>


Nous construisons le graphique des r√©sidus standardis√©s.
```{r}
rcn <-  (eps - mean(eps) )/  sqrt(as.numeric(sigma2_hat))
gr <- ggplot() + 
       geom_point( aes(x = modele$fitted.values, y = as.numeric(rcn)), colour = 'steelblue', size = 3) +
       geom_hline( yintercept = c(-1.96, 0 , +1.96), color = '#990000', linetype = 2 )                 +
       xlab('r√©sidus standardis√©s')
gr
```

On observe que les r√©sidus se pr√©sentent sous la forme d'une arche. On voit que certains r√©sidus sortent de la plage acceptable et d√©passent les lignes seuils, ce qui montre qu'il y a quelques points aberrants. 

Nous pouvons d√©sormais utiliser des outils exploratoires pour analyser notre mod√®le de donn√©es. 

Nous utilisons la fonction `ggpairs` du paquet `GGaly` afin de visualiser les nuages de points et les corr√©lations : 

```{r}
ggpairs(modele)
```

Nous pouvons voir dans la diagonale que nos donn√©es sont distribu√©s sous la forme d'une courbe sinusoidale ce qui est normal. 


Puis nous utilisons la fonction `corrplot` du paquet du m√™me nom pour visualiser les corr√©lations :
```{r}
corrplot(cor(df))
```

Lorsque deux variables sont corr√©l√©es elles varient ensemble. Si nous observons une corr√©lation positive, ceci signifie que les variables augmentent ensemble. Si la corr√©lation est n√©gative, les variables seront donc inversement proportionnelles.

On peut voir que `siri` et `densit√©` sont tr√®s fortement corr√©l√©s avec notre variable d'int√©r√™t `brozek`. Ces corr√©lations entra√Ænent une multicolin√©arit√© qui d√©stabilise le mod√®le. Nous avons donc bien fait de ne pas les inclure. 

Nous pouvons aussi voir d'autres corr√©lations entre d'autres variables:

- la variable `weight` est positivement corr√©l√©e avec `adipos`, `free`, `chest`, `abdom`, `thigh`, `biceps`
- la variable `adipos` est positivement corr√©l√©e avec `brozek`, `chest`, `abdom`, `thigh`, `biceps`
- la variable `chest` est positivement corr√©l√©e avec `brozek`, `neck`, `abdom`, `biceps`
- la variable `abdom` est positivement corr√©l√©e avec `brozek`
- la variable `thigh` est positivement corr√©l√©e avec `biceps`
- les variables `ankle` et `forearm` ne sont pas tr√®s corr√©l√©es avec d'autres variables

Les corr√©lations avec notre variable d'int√©r√™t `brozek` (`adipos`, `chest`, `abdom`) sont l√©g√®rement moins fortes que celles avec `siri` et `densit√©`, nous choisissons de les garder dans notre mod√®le, et de d√©cider plus tard avec d'autres tests si elles s'av√®rent probl√©matiques. 


<br> <hr>
Nous cherchons d√©sormais √† valider notre mod√®le.

Nous testons dans un premier temps l‚Äôind√©pendance des r√©sidus √† l‚Äôaide du test de **Durbin Watson**. Nous pourrons ainsi rep√©rer les autocorr√©lations positives entre les r√©sidus.
```{r}
dwtest(modele)
```

On peut voir que la p-value est inf√©rieure √† 0,05. Il y a donc une autocorr√©lation de nos r√©sidus. L'analyse par r√©gression multiple est donc instable. 

Puis nous voulons analyser les r√©sidus studentis√©s afin de d√©tecter les valeurs aberrantes.

Tout d'abord les r√©sidus internes:
```{r}
rstud<-rstudent(modele); indx  <-seq(1,length(rstud),1)
plot(rstud~indx,cex.lab =0.8,xlab='index',ylab='r√©sidus studentis√©s (standards)')
abline(h = c(-2,0,2),col ='red',lty=c(2,1,2))
abline(h = c(-2.8,2.8),col ='blue',lty=c(3,3))
```

On peut voir qu'on obtient des valeurs aberrantes tr√®s loin du seuil.
Nous cherchons √† les supprimer:

```{r}
aberrants <- abs(rstud) > 1.9
points_aberrants <- which(aberrants)
df_sans_aberrants <- df[-points_aberrants, ]
modele <- lm(brozek ~ weight + adipos + free + chest + abdom + thigh + ankle + biceps + forearm, data = df_sans_aberrants)

rstud<-rstudent(modele); indx  <-seq(1,length(rstud),1)
plot(rstud~indx,cex.lab =0.8,xlab='index',ylab='r√©sidus studentis√©s (standards)')
abline(h = c(-2,0,2),col ='red',lty=c(2,1,2))
abline(h = c(-2.8,2.8),col ='blue',lty=c(3,3))
```

Nous observons que nous avons beaucoup moins de valeurs aberrantes. 

Nous cherchons maintenant √† √©tudier les r√©sidus studentis√©s externes. Pour cela nous utilisons la fonction `hat` qui calcule la matrice chapeau et la fonction `model.matrix` qui transforme le dataframe en matrice.
```{r}
diag_hat <- hat(model.matrix(modele))
n<-length(diag_hat)                   
p<- length(modele[[1]])           
rstud_ext<-rstud*sqrt((n-p-1)/(n-p-rstud*rstud)) ;
indx  <-seq(1,length(rstud_ext),1)
plot(rstud_ext~indx,cex.lab =0.8,xlab='index',ylab='r√©sidus studentis√©s externes')
abline(h = c(-2,0,2),col ='red',lty=c(2,1,2))
abline(h = c(-2.8,2.8),col ='blue',lty=c(3,3))

```

Nous pouvons voir que m√™me si la majorit√© des valeurs se trouvent entre les lignes seuils, quelques valeurs d√©passent cet intervalle. Pour v√©rifier que les donn√©es sont malgr√© tout analysable, et qu'il s'agit de valeurs influentes et non pas aberrantes, nous utilisons la fonction `check_outliers`: 

```{r}
check_outliers(modele)
```

Nous voyons qu'il n'y a pas de valeurs aberrantes d√©tect√©es, donc nous pouvons confirmer notre hypoth√®se qu'il s'agissait de valeurs influentes.


Nous faisons ensuite les tests de **DFFITS** et **Cook**, qui permettent de d√©tecter les valeurs influentes sur l'estimation des valeurs pr√©dites.

Tout d'abord DFFITS est une technique utilisant la fonction `dffits.`
```{r}
# calcul des DFFITS
DFFITS<-dffits(modele) ; DFFITS <-abs(DFFITS)
# index des observations ayant une influence sur les valeurs pr?dites 
id_DFFITS<-which(abs(dffits(modele)) > 2*sqrt((p+1)/n))
#graphiques des DFFITS
plot(DFFITS,type ='h', ylab = 'abs(DFFITS)')
abline(h = 2*sqrt((p+1)/n),col ='red',lty=2)
```

Nous voyons qu'il y a 3 valeurs tr√®s influentes car elles sont bien au dessus du seuil. Elles se situent aux index de 49, 150 et 200.

Nous calculons ensuite la distance de **Cook** √† l'aide de la fonction `cooks.distance`.
```{r}
# Calcule de la distance de Cook
COOKS<-cooks.distance(modele)
plot(COOKS,type ='h', ylab = 'Distance de Cooks')
abline(h =  qf(0.05,n,n-p),col ='red',lty=2)
```

Nous pouvons revoir les valeurs que nous avions remarqu√© avec le **DFFITS**. 


Nous analysons ensuite les points leviers et dfbetas qui permettent de d√©tecter les valeurs influentes sur l'estimation des param√®tres. 
Tout d'abord les points leviers, sont des observations qui influencent l'estimation des param√®tres, on les obtient gr√¢ce √† :
```{r}
# points leviers
plot(diag_hat,type = 'h', ylab='points leviers',ylim=c(0,2*p/n+0.5))
abline(h = 2*p/n,col ='red',lty=2)
```

Nous pouvons observer 5 points leviers.

Les **DFBETAS** permettent de d√©tecter l'influence de la i √®me valeur sur le j √®me param√®tre. Nous les observons tel que : 
```{r}
# DFBETAS
par(mfcol = c(5, 2), mar = c(2, 2, 2, 2)) 
DFBETAS <- dfbetas(modele)
for(i in 1 : dim(DFBETAS)[2]) { plot(abs(DFBETAS[,i]), type = 'h', 
                                     ylab = paste('DFBETAS -> Variable : ', colnames(DFBETAS)[i]), cex.lab = 0.8)
                                abline(h = 2/sqrt(n), col = 'red', lty = 2)}
```

Nous observons 10 graphiques, dans chacun on voit certaines valeurs au dessus de la ligne seuil, qui sont donc influentes. On retrouve notamment nos 5 points leviers.

Nous utilisons le `covratio`, afin de d√©tecter les valeurs influentes sur la variance des param√®tres. 
```{r}
COVRATIO<-covratio(modele)
par(mfcol=c(1,1)) ; plot(abs(COVRATIO-1),type ='h', ylab = 'COVRATIO')
abline(h = 3*p/n,col ='red',lty=2)
```

On peut voir 6 valeurs qui sont bien au dessus du seuil et qui influencent donc la variance sur les param√®tres. Ces points affectent donc l'estimation de nos param√®tres de mani√®re significative. 


Enfin nous √©tudions la multicolin√©arit√© gr√¢ce au test du **VIF**.
```{r}
v<-vif(modele)
barplot(v,type ='h',cex.lab = 0.9, ylab = 'VIF')
abline(h = 5,col ='red',lty=2)
```

On peut voir que les variables sont presque toutes au dessus du seuil de VIF, il y a donc de nombreuses multicolin√©arit√©s rendant notre mod√®le instable et donc non analysable. 

Nous finissons par confirmer que nous ne pouvons pas analyser notre mod√®le avec la commande `check_model` qui est un r√©sum√© de tests sur le mod√®le:
```{r}
check_model(modele)
```

Sur ce graphique nous voyons en condens√© les caract√©ristiques de notre mod√®le. En haut √† gauche nous voyons la v√©rification pr√©dictive post√©rieure qui v√©rifie si le mod√®le est efficace dans la pr√©diction des variables y observ√©es. Nous voyons que la courbe obtenue(bleu) suit la courbe pr√©dite (verte), le mod√®le est donc bien capable de reproduire les caract√©ristiques observ√©es dans les donn√©es r√©elles.

En haut √† droite nous retrouvons la lin√©arit√©, qui nous permet donc de v√©rifier la relation lin√©aire entre l'ensemble des variables explicatives et la variable expliqu√©e. Nous voyons que ce que nous obtenons n'est pas exactement une droite, car elle est l√©g√®rement arch√©e Cependant, la ligne s'affiche en vert malgr√© tout, nous faisons donc l'hypoth√®se que m√™me si elle n'est pas parfaite, la lin√©arit√© du jeu de donn√©e est acceptable.

Nous retrouvons dans la ligne suivante l'homog√©n√©it√© de la variance, qui nous permet de v√©rifier la condition d'homosc√©dasticit√© du jeu de donn√©es. Nous voyons que la ligne obtenue devrait √™tre plate et horizontale, et correspond (encore une fois √† peu pr√®s) √† l'allure de la courbe que nous obtenons. Nous n'avons donc pas de probl√®me d'h√©t√©rosc√©dasticit√©.

√Ä c√¥t√© nous voyons les points influents du mod√®le, qui sont bien contenu entre les lignes seuils, nous n'avons donc plus de valeurs aberrantes, seulement des valeurs influentes.

Juste en dessous, on retrouve le graphique de la normalit√© des r√©sidus, soit la v√©rification de la distribution normale des r√©sidus (qui fonctionne donc comme un QQ plot). Les points sont r√©partis sur une ligne horizontale, sauf aux extr√©mit√©s. Cependant, nous avons consid√©r√© que l'allure de la courbe √©tait suffisante pour conclure qu'il n'y avait pas de probl√®me de normalit√©.

Le dernier graphique, en bas √† gauche, repr√©sente la colin√©arit√©. Il s'agit d'une autre repr√©sentation du VIF, o√π nous remarquons que nous avons donc bien un probl√®me de multicolin√©arit√©.


Nous ne pouvons donc pas analyser nos donn√©es √† partir de la r√©gression lin√©aire multiple, nous allons donc essayer d'utiliser la r√©gression p√©nalis√©e.





<hr style="border: 1px solid #EE50A3;">


#### <FONT color='#EE50A3'> III. R√©gression p√©nalis√©e  </FONT>


Pour r√©soudre les probl√®mes de multicolin√©arit√© et am√©liorer l'interpr√©tation de nos mod√®les, nous avons recours √† des mod√®les de r√©gression p√©nalis√©e. Ces techniques permettent de contr√¥ler la complexit√© du mod√®le en ajoutant une p√©nalit√© aux coefficients. Parmi les approches disponibles, nous explorons **Ridge** et **Lasso**.

Nous observons nos donn√©es :
```{r}
y_name <- 'brozek'

Y  <- fat %>% select(  all_of(y_name))
X <- fat %>% select(-all_of(c(y_name, "siri", "density")))
XY <- fat

n_obs <- nrow(X)

X_stand  <- as_tibble(scale(X))
Y_stand  <- as_tibble(scale(Y, center = T, scale = F))
XY_stand <- X_stand %>% bind_cols(Y_stand) 

#->dans la formulation, on introduit le 0 
#  pour signifier que l'on ne doit pas calculer l'ordonn√©e √† l'origine !!
formul   <- as.formula(paste0(y_name , ' ~  + 0 + ', paste0(colnames(X), collapse = ' + ')))
```
Nous choisissons d'exclure √† nouveau `siri` et `densit√©` de l'√©tude, car ces variables sont tr√®s fortement corr√©l√©es avec la variable expliqu√©e et risquent donc de parasiter nos r√©sultats.

```{r, echo = F}
fat  %>% kbl(digits=3) %>%  kable_styling(bootstrap_options = "striped", full_width = F, position = "center", latex_options = 'stripped') %>% scroll_box( height = "250px")  
#-> r√©gression lin√©aire multiple
model_simple <- lm(formula = formul, data = XY_stand)
```

##### <u><FONT color='#8A226'>**1. R√©gression Ridge**</FONT></u>


   La r√©gression Ridge \( L2 \) minimise l‚Äôexpression suivante :
   \[
   \min_{\Theta} \left( \sum_{i=1}^{n} (y_i - X_i \Theta)^2 + \lambda \sum_{j=1}^{p} \theta_j^2 \right)
   \]
   o√π \( \lambda \) est un hyperparam√®tre qui contr√¥le l‚Äôimportance de la p√©nalit√© appliqu√©e √† la norme \( L2 \) des coefficients. Cela a pour effet de r√©duire les valeurs des coefficients sans les annuler compl√®tement, rendant le mod√®le moins sensible aux corr√©lations √©lev√©es entre les variables explicatives.

Code explicatif pour la r√©gression Ridge:


On charge les donn√©es depuis un fichier Excel nomm√© `FAT.xlsx` situ√© dans le r√©pertoire d√©fini par la fonction **here()**. Et on d√©finit une graine pour assurer la reproductibilit√© des r√©sultats al√©atoires. 

On cr√©e une matrice X contenant les pr√©dicteurs s√©lectionn√©s. Ces variables repr√©sentent les caract√©ristiques li√©es aux individus dans l'√©tude (√¢ge, poids, taille, etc.).
On g√©n√®re une variable cible y comme la somme pond√©r√©e des 15 premi√®res colonnes de X,√† laquelle est ajout√© un bruit gaussien al√©atoire pour simuler une r√©ponse. 

On standardise les pr√©dicteurs (centrage √† z√©ro et mise √† l‚Äô√©chelle √† l‚Äô√©cart-type unitaire) pour √©viter que les variables √† grande √©chelle n‚Äô√©crasent les autres. La normalisation des donn√©es de test utilise les param√®tres de centrage et d'√©chelle des donn√©es d'entra√Ænement.

`lambda` g√©n√®re une s√©quence de valeurs pour r√©gulariser le mod√®le et `cv.glmnet` ajuste un mod√®le Ridge et effectue une validation crois√© pour trouver la valeur de lambda la plus optimale.
`lambda.min` permet d'identifier la valeur de lambda qui minimise MSE et `predict` utilise le mod√®le ajust√© pour faire des pr√©dictions sur les donn√©es de test. 

Enfin on calcule l'erreur quadratique moyenne (MSE) entre les pr√©dictions et les valeurs observ√©es de y.
   
```{r, echo = T}
set.seed(123)
#-> progression g√©om√©trique des valeurs possibles de lambda
lambda   <- 10^seq(-3,5,length.out = 100)

#-> regression ridge et validation crois√©e
# .... la standardisation = F car les donn√©es sont d√©j√† standardis√©es
# les arguments x et y doivent √™tre des matrices et non des data frames ni des tibbles
ridge_cv <- cv.glmnet(x = as.matrix(X_stand), y = as.matrix(Y_stand), lambda = lambda, standardize = F, nfolds = 10, alpha = 0)

#-> r√©sultats SCE
plot(ridge_cv)

id_best_ridge  <- which.min(ridge_cv$cvm)
lam_best_ridge <- ridge_cv$lambda[id_best_ridge]
MSE_best_ridge <- ridge_cv$cvm[id_best_ridge]

paste0(' Ridge : best_lam = ' ,lam_best_ridge, '  --- Best MSE = ', MSE_best_ridge)

```

On voit que le mod√®le commence par avoir une MSE proche de 10, puis en augmentant le lambda et donc la p√©nalit√©, la MSE diminue puis r√©-augmente jusqu‚Äô√† stabilisation quand la MSE est maximale.
Nous choisissons le meilleur lambda, soit celui qui minimise l‚Äôerreur, donc situ√© au minimum de la courbe. Celui ci est mis en valeur entre les barres verticales, il est donc l√©g√®rement inf√©rieur √† 0.


##### <FONT color='#CF4594'> On effectue une estimation des param√®tres </FONT> 

`glmnet` ajuste les coefficients Œò pour chaque valeur de Œª dans l‚Äôensemble d‚Äôentra√Ænement.
`param_best_ridge` calcule les coefficients sp√©cifiques au Œª optimal.

On formate les coefficients des mod√®les Ridge en fonction des valeurs logarithmiques de Œª pour permettre la visualisation graphique.

On visualise les coefficients des pr√©dicteurs en fonction de log(ùúÜ).
```{r, echo = T}

#--> estimation des param√®tres en fonction des lambdas
param_ridge <- glmnet(x = as.matrix(X_stand), y = as.matrix(Y_stand), lambda = lambda, standardize = F,  alpha = 0)

#-> estimation des param√®tres pour le lambda optimal
param_best_ridge <- glmnet(x = as.matrix(X_stand), y = as.matrix(Y_stand), lambda = lam_best_ridge, standardize = F,  alpha = 0)

#-> reformulation du graphique
df_param <- data.frame(as.matrix(t(param_ridge$beta)), rev(log(lambda)))  ; colnames(df_param) <- c(colnames(X), 'lam')
df_melt  <- melt(df_param, id.vars = 'lam')
gr_ridge <- ggplot() + geom_line(data = df_melt, aes(x = lam, y = value, colour = variable), size = 0.5)  +
                    xlab('log(lambda)') + ylab('Coefficients') +
                    geom_vline(aes(xintercept = lam_best_ridge), colour = '#993300', size = 0.5, linetype = 1)
gr_ridge

print(param_best_ridge$beta)

```

On voit que les variables `weight` et `free` sont les plus influentes sur le calcul de notre lambda, car les coefficients en valeur absolue sont tr√®s √©lev√©s (=10) et donc tr√®s √©loign√©s de 0. Une ligne verticale marque la valeur optimale de Œª, o√π les coefficients sont les mieux √©quilibr√©s entre biais et variance. On voit √† cette abscisse, que les variables ont toute un effet √† peu pr√®s √©quilibr√©, la plus importante √©tant `free` car il s'agit de la plus √©loign√©e de 0.



##### <FONT color='#CF4594'> Comparaison regression multiple et Ridge </FONT>

```{r}
Y_est_ridge <- as.matrix(X_stand) %*% as.matrix(param_best_ridge$beta) 
Y_est_lm    <- model_simple$fitted.values
Y_obs       <- Y_stand
temp        <- data.frame(Y_est_ridge,Y_est_lm) ; names(temp) <- c('ridge', 'glm')
temp        <- melt(temp)
df_comp     <- data.frame (temp, 'y_obs' =  rep(as.matrix(Y_obs,ncol = 1),2))

gr_comp1    <- ggplot(data = df_comp, aes(x = y_obs, y = value, colour = variable)) + geom_point() + 
               geom_smooth(method = lm, se = F, size = 0.5)
gr_comp1

```

Le mod√®le de r√©gression Ridge a permis de r√©duire l'impact de la multicolin√©arit√©, avec des coefficients qui tendent vers z√©ro √† mesure que Œª augmente.

En effet, on voit dans ce graphique l'effet de **Ridge** par rapport √† la r√©gression lin√©aire multiple. La droite rouge passe bien par les points rouges correspondant √† la m√©thode de **Ridge** donc on comprend que le mod√®le est performant. Cependant autour de la droite bleu mod√©lisant la r√©gression lin√©aire multiple (glm) les points sont assez dispers√©s.
Les points associ√©s au mod√®le Ridge sont moins dispers√©s ou mieux align√©s que ceux du **GLM**, cela montre que **Ridge** g√®re mieux les probl√®mes de multicolin√©arit√© gr√¢ce √† sa r√©gularisation.

En comparaison avec la r√©gression lin√©aire classique, la r√©gression **Ridge** a montr√© une plus grande robustesse face aux variables corr√©l√©es, offrant ainsi des pr√©dictions plus fiables. Les coefficients ont √©t√© stabilis√©s, ce qui r√©duit le risque de surajustement tout en maintenant les variables les plus pertinentes.


---

##### <u><FONT color='#8A226'>**2. R√©gression Lasso**</FONT></u>


   La r√©gression Lasso ajoute une p√©nalit√© \( L1 \), minimisant ainsi :
   \[
   \min_{\Theta} \left( \sum_{i=1}^{n} (y_i - X_i \Theta)^2 + \lambda \sum_{j=1}^{p} |\theta_j| \right)
   \]
  
  Cette m√©thode permet de r√©duire certains coefficients √† z√©ro, ce qui simplifie le mod√®le et le rend plus interpr√©table. La r√©gression **Lasso** est particuli√®rement utile lorsque l'on travaille avec un grand nombre de variables, car elle permet d'√©liminer celles qui ne sont pas pertinentes tout en maintenant une bonne performance du mod√®le.
  
  Le code standardise les donn√©es avant d'appliquer la r√©gression **Lasso** avec la fonction `cv.glmnet`. Cette fonction effectue une validation crois√©e pour d√©terminer la valeur optimale de Œª. Un graphique est g√©n√©r√© pour visualiser l'√©volution de l'erreur quadratique moyenne (MSE) en fonction de Œª.
   
```{r}
lasso_cv <- cv.glmnet(x = as.matrix(X_stand), y = as.matrix(Y_stand), lambda = lambda, standardize = F, nfolds = 10, alpha = 1)
plot(lasso_cv)

id_best_lasso  <- which.min(lasso_cv$cvm)
lam_best_lasso <- lasso_cv$lambda[id_best_lasso]
MSE_best_lasso <- lasso_cv$cvm[id_best_lasso]

paste0(' Lasso : best_lam = ' ,lam_best_lasso, '  --- Best MSE = ', MSE_best_lasso)

```

On voit que le mod√®le commence par avoir une MSE proche de 10, puis en augmentant le lambda et donc la p√©nalit√©, la MSE diminue puis r√©-augmente jusqu‚Äô√† stabilisation quand la MSE est maximale.
Nous choisissons le meilleur lambda, soit celui qui minimise l‚Äôerreur, donc situ√© au minimum de la courbe. Celui ci est mis en valeur entre les barres verticales, il est donc l√©g√®rement inf√©rieur √† 0. Nous observons √©galement que la pente est plus abrupte, donc que le coefficient directeur est plus √©lev√© pour la m√©thode Lasso que pour la m√©thode Ridge.


##### <FONT color='#CF4594'> Estimation des param√®tres </FONT>

Ensuite, on estime les param√®tres pour diff√©rentes valeurs de Œª et trace un graphique pour montrer comment les coefficients √©voluent avec Œª. On a √©galement extrait et affich√© les param√®tres pour la valeur optimale de Œª.

```{r, echo = T}
#--> estimation des param√®tres en fonction des lambdas
param_lasso <- glmnet(x = as.matrix(X_stand), y = as.matrix(Y_stand), lambda = lambda, standardize = F,  alpha = 1)

#-> estimation des param√®tres pour le lambda optimal
param_best_lasso <- glmnet(x = as.matrix(X_stand), y = as.matrix(Y_stand), lambda = lam_best_lasso, standardize = F,  alpha = 1)

#-> reformulation du graphique
df_param_lasso <- data.frame(as.matrix(t(param_lasso$beta)), rev(log(lambda)))  ; colnames(df_param_lasso) <- c(colnames(X), 'lam')
df_melt_lasso  <- melt(df_param_lasso, id.vars = 'lam')
gr_ridge <- ggplot() + geom_line(data = df_melt_lasso, aes(x = lam, y = value, colour = variable), size = 0.5)  +
                    xlab('log(lambda)') + ylab('Coefficients') +
                    geom_vline(aes(xintercept = lam_best_lasso), colour = '#993300', size = 0.5, linetype = 1)
gr_ridge
print(param_best_lasso$beta)
```

Nous pouvons voir dans le tableau des meilleurs param√®tres que ceux de l'`age`, `adipos`, `neck`, `hip` et `wrist` sont d√©sormais √©gal √† 0. Ces variables ont √©t√© consid√©r√© comme moins importantes et ont donc √©t√© simplifi√©es du mod√®le. Nous voyons graphiquement que `weight`et `free` sont toujours les variables qui ont le plus d'influence sur notre mod√®le, car elles ont les coefficients les plus importants.


##### <FONT color='#CF4594'> Comparaison r√©gression lin√©aire multiple, Ridge et Lasso </FONT>

Enfin, on compare les r√©sultats obtenus avec la r√©gression **Lasso**, la r√©gression **Ridge**, et la r√©gression lin√©aire classique. Cela nous permet de voir que le mod√®le **Lasso**, en r√©duisant certains coefficients √† z√©ro, aboutit √† un mod√®le plus simple, tout en conservant une bonne capacit√© pr√©dictive.

```{r}
Y_est_lasso <- as.matrix(X_stand) %*% as.matrix(param_best_lasso$beta) 
Y_est_ridge <- as.matrix(X_stand) %*% as.matrix(param_best_ridge$beta) 
Y_est_lm    <- model_simple$fitted.values
Y_obs       <- Y_stand
temp        <- data.frame(Y_est_lasso, Y_est_ridge, Y_est_lm) ; names(temp) <- c('lasso', 'ridge', 'glm')
temp        <- melt(temp)
df_comp     <- data.frame (temp, 'y_obs' =  rep(as.matrix(Y_obs,ncol = 1),3))

gr_comp2    <- ggplot(data = df_comp, aes(x = y_obs, y = value, colour = variable)) + geom_point() + 
               geom_smooth(method = lm, se = F, size = 0.5)
gr_comp2
```


Le mod√®le de r√©gression **Lasso** a permis de s√©lectionner les variables les plus pertinentes en r√©duisant certains coefficients √† z√©ro gr√¢ce √† la p√©nalisation \( L1 \).

En effet, on voit dans ce graphique l'effet de **Lasso** par rapport √† la r√©gression lin√©aire multiple et √† **Ridge**. La droite correspondant √† Lasso est bien align√©e avec les points associ√©s √† cette m√©thode, montrant une bonne capacit√© pr√©dictive. Comparativement, les points de la r√©gression lin√©aire multiple (**GLM**) sont davantage dispers√©s autour de leur droite de tendance, ce qui indique un risque plus √©lev√© de surajustement. 

Les points li√©s √† **Lasso** sont √©galement moins dispers√©s que ceux de **GLM** et, dans certains cas, plus align√©s que ceux de **Ridge**. Cela refl√®te l'avantage de **Lasso** en termes de simplification du mod√®le, en √©liminant les variables inutiles tout en conservant une performance pr√©dictive √©lev√©e.

 En comparaison, **Lasso** offre un mod√®le plus interpr√©table que **Ridge** gr√¢ce √† la suppression des coefficients non significatifs, tout en conservant une robustesse face √† des donn√©es complexes ou bruit√©es.


---
   
##### <u><FONT color='#8A226'>**3. ElasticNet**</FONT></u>


La m√©thode **ElasticNet** combine les avantages des r√©gressions **Ridge** et **Lasso**. Elle permet de s√©lectionner les variables non pertinentes tout en offrant une solution robuste en cas de multicolin√©arit√© √©lev√©e entre les variables explicatives. Contrairement √† **Lasso** qui peut √©liminer certaines variables de mani√®re trop stricte, **ElasticNet** permet de partager les poids entre les variables fortement corr√©l√©es, ce qui en fait une m√©thode plus flexible et adapt√©e pour des mod√®les avec des interactions complexes entre les variables.


  Le crit√®re √† minimiser pour l‚Äôestimation des param√®tres est le suivant :  
\[
J(\hat{\Theta}_{enet}) = \left[ \frac{1}{n} \sum_{i=1}^{n} \left( y_i - \sum_{j=1}^{p} \hat{\theta}_j x_{i,j} \right)^2 \right] + \lambda \left[ \alpha \sum_{j=1}^{p} |\hat{\theta}_j| + (1-\alpha) \sum_{j=1}^{p} \hat{\theta}_j^2 \right]
\]

  o√π :  
- \( \lambda \) est le facteur de p√©nalisation,  
- \( \alpha \) est le facteur de m√©lange entre Lasso (\( L1 \)) et Ridge (\( L2 \)).

  Les param√®tres \( \lambda \) et \( \alpha \) sont estim√©s via **validation crois√©e**.

  La condition optimale pour \( \hat{\Theta}_{enet} \) est obtenue en annulant le gradient :  
\[
\frac{\partial J(\hat{\Theta}_{enet})}{\partial \hat{\Theta}_{enet}} = 0
\]

  **ElasticNet** est particuli√®rement utile pour :  
    - R√©duire la complexit√© des mod√®les tout en √©vitant la        s√©lection arbitraire,  
    - Maintenir la robustesse en cas de multicolin√©arit√©          √©lev√©e entre les variables explicatives.  
    

Les donn√©es sont d'abord standardis√©s pour garantir que chaque variable soit √† la m√™me √©chelle. Ensuite, un ensemble de valeurs pour Œª et Œ± est d√©fini. 
Œª contr√¥le la p√©nalit√© de r√©gularisation, tandis que Œ± ajuste le compromis entre la r√©gularisation de type **Lasso** (L1) et **Ridge** (L2). 

La fonction `cv.glmnet` est utilis√©e pour effectuer une validation crois√©e et d√©terminer les meilleurs param√®tres Œª et Œ± en minimisant l'erreur quadratique moyenne (MSE). 

Une fois les meilleurs param√®tres trouv√©s, le mod√®le **ElasticNet** est ajust√© avec ces valeurs optimales.


```{r}
lambda  <- 10^seq(-3,5,length.out = 100)
alpha   <- seq(0,1,length.out = 100)  

n_tune  <- length(alpha)
tune    <- data.frame(mse = rep(0,n_tune), lambda = rep(0,n_tune), alpha = rep(0,n_tune))

for (i in 1:n_tune)
      {
        elastic_cv       <- cv.glmnet(x = as.matrix(X_stand), y = as.matrix(Y_stand), lambda = lambda, standardize = F, nfolds = 10, alpha = alpha[i])
        id_best_elastic  <- which.min(elastic_cv$cvm)
        tune[i,1]        <- elastic_cv$cvm[id_best_elastic]
        tune[i,2]        <- elastic_cv$lambda[id_best_elastic]
        tune[i,3]        <- alpha[i]

      }

    
    id <- which.min(tune[,1])              
    alpha_best       <- tune[id,3]
    lam_best_elastic <- tune[id,2]
    MSE_best_elactic <- tune[id,1]
    param_elastic    <- glmnet(x = as.matrix(X_stand), y = as.matrix(Y_stand), lambda = tune[id,2], standardize = F, nfolds = 10, alpha = tune[id,3])

    paste0(' Elasticnet : best_lam = ' ,lam_best_elastic, '  ---- Best alpha = ', alpha_best,'  --- Best MSE = ', MSE_best_elactic)  
    
param_elastic$beta

```

Nous voyons que ici les param√®tres `age`, `adipos`, `hip` et `wrist` ont √©t√© mis a 0 tout comme lors de la r√©gression **Lasso**. Cependant, malgr√© la ressemblance, les valeurs des param√®tres ont l√©g√®rement chang√© entre **Lasso** et **ElasticNet**. N√©anmoins, on voit une grande diff√©rence par rapport aux valeurs des param√®tres du mod√®le **Ridge**.


##### <FONT color='#CF4594'> Comparaison entre la regression lin√©aire multiple, Ridge, Lasso et ElasticNet </FONT>

```{r}
Y_est_elastic  <- as.matrix(X_stand) %*% as.matrix(param_elastic$beta)
temp        <- data.frame(Y_est_lasso, Y_est_ridge, Y_est_elastic, Y_est_lm) ; names(temp) <- c('lasso', 'ridge', 'elasticnet', 'glm')
temp        <- melt(temp)
df_comp     <- data.frame (temp, 'y_obs' =  rep(as.matrix(Y_obs,ncol = 1),4))

gr_comp1    <- ggplot(data = df_comp, aes(x = y_obs, y = value, colour = variable, shape = variable)) + geom_point() + 
               geom_smooth(method = lm, se = F, size = 0.5)
gr_comp1

```

Cette figure montre une comparaison entre les valeurs observ√©es et les valeurs estim√©es pour les mod√®les de r√©gression lin√©aire multiple **GLM**, **Ridge**, **Lasso**, et **ElasticNet**. Chaque m√©thode est repr√©sent√©e par des points color√©s et une droite de tendance ajust√©e. L‚Äôaxe horizontal correspond aux valeurs observ√©es, tandis que l‚Äôaxe vertical indique les valeurs pr√©dites. On peut remarquer que la dispersion des points autour des droites varie selon les mod√®les.

Les points associ√©s au mod√®le **GLM** sont les plus dispers√©s, notamment lorsque les valeurs s‚Äô√©loignent de la moyenne, ce qui montre que ce mod√®le est moins robuste face √† la multicolin√©arit√© ou au bruit. √Ä l‚Äôinverse, les points des mod√®les **Ridge**, **Lasso**, et **ElasticNet** sont mieux align√©s avec leurs droites respectives. **ElasticNet**, en particulier, semble √©quilibrer la pr√©cision de Ridge tout en b√©n√©ficiant de la simplification offerte par **Lasso**.

 Les points moins dispers√©s autour de la droite **ElasticNet** confirment que ce mod√®le combine fiabilit√© et adaptabilit√©, ce qui en fait une m√©thode robuste pour des donn√©es complexes.

##### <FONT color='#CF4594'> Comparaison des r√©sultats</FONT></u>

```{r}
MSE_lm <- 1/ n_obs * t(matrix(model_simple$residuals, ncol =1)) %*% matrix(model_simple$residuals, ncol = 1)
MSE  <- c('lm' = MSE_lm, 'ridge' = MSE_best_ridge, 'lasso'= MSE_best_lasso, 'elastic' = MSE_best_elactic)
MSE

```
On peut voir que l'erreur a augment√© par rapport √† la r√©gression lin√©aire multiple, car la r√©gression p√©nalis√©e ajoute une l√©g√®re erreur.

Dans le mod√®le **Ridge**, la valeur optimale de Œª, identifi√©e par validation crois√©e, minimise l'erreur quadratique moyenne (MSE) sur les donn√©es de test, ce qui montre que ce mod√®le est plus performant que la r√©gression multiple.

Dans le mod√®le **Lasso**, l'erreur quadratique est de nouveau minimis√©e de plus la suppression des coefficients non significatifs permet d'augmenter la pr√©cision. Ce mod√®le a une MSE inf√©rieure √† la r√©gression multiple et **Ridge**, et est donc plus pr√©cis.

Dans le mod√®le **ElasticNet**, nous r√©alisons un compromis efficace entre les m√©thodes **Lasso** et **Ridge**
Nous pouvons voir que l'erreur ajout√©e la plus faible est celle du mod√®le **ElasticNet**, c'est donc logiquement le meilleur mod√®le car il s√©lectionne les variables importantes tout en g√©rant efficacement la multicolin√©arit√©.


```{r}
All_param <- data.frame(as.matrix(cbind( model_simple$coefficients, param_best_ridge$beta, param_elastic$beta, param_best_lasso$beta )))
colnames(All_param)<- c('lm','ridge', "elastic", "lasso")
All_param
```


Les coefficients obtenus dans `All_param` montrent que **GLM** a des coefficients non r√©gularis√©s, √©lev√©s ce qui indique un surajustement. 
**Ridge** r√©duit l'amplitude des coefficients en conservant toutes les variables. 
**Lasso** √©limine certains coefficients en les ramenant √† 0 ce qui simplifie le mod√®le.
Enfin **ElasticNet** a √©quilibr√© la r√©duction des coefficents et la s√©lection des variables ce qui maintient les coefficients faibles mais pertinents.
   

<hr style="border: 1px solid #EE50A3;">

#### <FONT color='#EE50A3'> IV. Conclusion </FONT>

Ce projet nous a permis d‚Äôapprofondir notre compr√©hension des m√©thodes de r√©gression lin√©aire multiple et des mod√®les p√©nalis√©s (**Ridge** et **Lasso**) √† travers l‚Äôanalyse du jeu de donn√©es : `FAT` (pour pr√©dire le pourcentage de graisse corporelle). L‚Äôobjectif principal √©tait de construire des mod√®les fiables, d‚Äôinterpr√©ter leurs coefficients et d‚Äô√©valuer leur capacit√© pr√©dictive tout en tenant compte des √©ventuels d√©fis comme la multicolin√©arit√© et le surajustement.

Nous avons donc pu voir tout d'abord que la r√©gression lin√©aire multiple n'√©tait pas adapt√©e √† l'analyse de ce jeu de donn√©es, √† cause de la multicolin√©arit√© des variables. Puis nous avons pu analyser la diff√©rence entre les 3 types de r√©gressions p√©nalis√©es. Nous en avons d√©duit que la r√©gression la plus pertinente pour pr√©dire des valeurs de `brozek` est l'**ElasticNet.**

Cependant nous voyons que l'erreur MSE est toujours l√©g√®rement haute, par rapport √† celle de la r√©gression lin√©aire multiple. Il s'agit d'une des limites de la r√©gression p√©nalis√©e.

Ce projet a renforc√© nos comp√©tences en programmation sous R, notamment dans l‚Äôapplication pratique des mod√®les statistiques. On a √©galement acquis une meilleure ma√Ætrise des outils de visualisation et appris √† tirer parti des mod√®les p√©nalis√©s pour r√©soudre des probl√®mes complexes.

Enfin, nous r√©alisons que la qualit√© de la communication des r√©sultats (graphiques, tableaux, conclusions claires) est aussi essentielle que l‚Äôanalyse elle-m√™me.

<hr style="border: 1px solid #EE50A3;">

#### <FONT color='#EE50A3'>V. Bibliographie</FONT>  

**R√©partition des t√¢ches:** 
Eve s'est occup√©e de la partie code de la r√©gression lin√©aire multiple ainsi que de son code markown.

L√©hna s'est occup√©e de la partie r√©gression p√©nalis√©e ainsi que de son code markdown.

Nous avons effectu√© ensemble l'interpr√©tation des r√©sultats ainsi que les recherches bibliographiques. 


  1. **Documentation des packages R utilis√©s**. Disponible sur CRAN : [https://cran.r-project.org/web/packages/available_packages_by_name.html](https://cran.r-project.org/web/packages/available_packages_by_name.html).  
  2. **James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013).** *An Introduction to Statistical Learning: with Applications in R*. Springer. Disponible gratuitement en PDF sur : [https://www.statlearning.com/](https://www.statlearning.com/).  
  3. **Tutoriel Markdown**. *Introduction au Markdown par RStudio*. Vid√©o disponible sur : [Bing Videos](https://www.bing.com/videos/riverview/relatedvideo?q=Introduction+au+Markdown+par+RStudio+tutoriel&mid=1B28DB169E035C134D081B28DB169E035C134D08&FORM=VIRE).  
  4. **DellaData**. *Guide de d√©marrage en R markdown*. Disponible sur : [https://delladata.fr/guide-de-demarrage-en-r-markdown/](https://delladata.fr/guide-de-demarrage-en-r-markdown/).   
  5. StatQuest with Josh Starmer - Ridge, Lasso and Elastic-Net Regression in R   https://youtu.be/ctmNq7FgbvI?si=ESzl3UQOZvmwF4xn
  6. **DellaData**. *Fonctions math√©matiques en LaTeX pour R Markdown*. Disponible sur : [https://delladata.fr/equations-latex-rmarkdown/](https://delladata.fr/equations-latex-rmarkdown/).
  7. MODELES_LINEAIRE__2023.pdf. Disponible sur Blackboard.
  8. **REG_LIN_SIMPLE_TRAME_MARKDOWN.Rmd**. Disponible sur Blackboard.  
  9. **REGRESSION_LINEAIRE_MULTIPLE_2023.html**. Disponible sur Blackboard.  
  10. **REG_PENALISEE_2023.html**. Disponible sur Blackboard.  
  11. **REGRESSION_LINEAIRE_MULTIPLE_2022.Rmd**. Disponible sur Blackboard.  
  12. **REG_PENALISEE_2023.Rmd**. Disponible sur Blackboard.  
  13. **DellaData**. *Tutoriel: la r√©gression lin√©aire multiple avec R*. Disponible sur :https://delladata.fr/tutoriel-regression-lineaire-multiple-r/  
  <hr style="border: 1px solid #EE50A3;">